#!/usr/bin/python -u
import argparse
import json
import os
import subprocess
import tempfile
from datetime import datetime
from itertools import cycle
from shutil import get_terminal_size
from threading import Thread
from time import sleep
from typing import TypedDict

import requests
from rich.console import Console
from rich.markdown import Markdown

app_name = "ai-cli"


class Loader:
    def __init__(self, desc="Loading...", end="Done!", timeout=0.1):
        """
        A loader-like context manager

        Args:
            desc (str, optional): The loader's description. Defaults to "Loading...".
            end (str, optional): Final print. Defaults to "Done!".
            timeout (float, optional): Sleep time between prints. Defaults to 0.1.
        """
        self.desc = desc
        self.end = end
        self.timeout = timeout

        self._thread = Thread(target=self._animate, daemon=True)
        self.steps = ["⢿", "⣻", "⣽", "⣾", "⣷", "⣯", "⣟", "⡿"]
        self.done = False

    def start(self):
        self._thread.start()
        return self

    def _animate(self):
        for c in cycle(self.steps):
            if self.done:
                break
            print(f"\r{self.desc} {c}", flush=True, end="")
            sleep(self.timeout)

    def __enter__(self):
        self.start()

    def stop(self):
        self.done = True
        cols = get_terminal_size((80, 20)).columns
        print("\r" + " " * cols, end="", flush=True)
        print(f"\r{self.end}", flush=True)

    def __exit__(self, exc_type, exc_value, tb):
        self.stop()


def get_config_path():
    path = os.path.expandvars(f"$HOME/.config/{app_name}")
    if not os.path.exists(path):
        os.makedirs(path)
    return path


def get_conversation_history_path():
    path = get_config_path() + "/history_conversation"
    if not os.path.exists(path):
        with open(path, "+w") as file:
            file.write("[]")
    return path


def get_history_path():
    path = get_config_path() + "/history"
    if not os.path.exists(path):
        open(path, "+a").close()
    return path


def get_api_key_path():
    path = get_config_path() + "/api_key"
    if not os.path.exists(path):
        open(path, "a").close()
    return path


def get_api_key():
    key = ""
    with open(get_api_key_path(), "r") as f:
        key = f.read()
    key_env = os.environ.get("OPEN_ROUTER_API_KEY")
    if key_env is not None:
        key = key_env
    if key == "":
        print(
            f"Open router api not provided. export OPEN_ROUTER_API_KEY=your_key, or {app_name} -c your_key"
        )
        exit(0)
    return key


class HistoryElement(TypedDict):
    question: str
    response: str


def select_attitude(role: str) -> str:
    match role:
        case "backend-developer-concise":
            return "you are the assistant of a senior backend developer. Answer with markdown. Be concise please, unless told otherwise."
        case "backend-developer":
            return "you are the assistant of a senior backend developer. Answer with markdown."
        case "none":
            return ""
    return role


def select_model(model: str) -> str:
    if model == "claude":
        return "anthropic/claude-3.7-sonnet:beta"
    if model == "openai":
        return "openai/gpt-4o"
    if model == "codestral":
        return "mistralai/codestral-2501"
    return model


def ai_cli(
    question: str, model: str, attitude: str, history: list[HistoryElement] | None
):
    url = "https://openrouter.ai/api/v1/chat/completions"
    attitude = attitude
    headers = {
        "Authorization": f"Bearer {get_api_key()}",
        "Content-Type": "application/json",
    }
    messages = []
    if attitude != "":
        messages.append({"role": "user", "content": attitude})
    if history is not None:
        for element in history:
            messages.append({"role": "user", "content": element["question"]})
            messages.append({"role": "assistant", "content": element["response"]})
    messages.append({"role": "user", "content": question})
    payload = {
        "model": model,
        "messages": messages,
        "stream": True,
    }
    buffer = ""
    response = ""

    print()
    loader = Loader(f"getting response from {model}...", "", 0.10).start()
    with requests.post(url, headers=headers, json=payload, stream=True) as r:
        for chunk in r.iter_content(chunk_size=1024, decode_unicode=True):
            buffer += chunk
            while True:
                try:
                    line_end = buffer.find("\n")
                    if line_end == -1:
                        break
                    line = buffer[:line_end].strip()
                    buffer = buffer[line_end + 1 :]
                    if line.startswith("data: "):
                        data = line[6:]
                        if data == "[DONE]":
                            break
                        try:
                            data_obj = json.loads(data)
                            content = data_obj["choices"][0]["delta"].get("content")
                            if content:
                                # print(content, end="", flush=True)
                                response += content
                        except json.JSONDecodeError:
                            pass
                except Exception:
                    break
    loader.stop()
    return response


def read_question_from_file(file: str) -> str:
    if not os.path.exists(file):
        print(
            f"file {file} does not exitst. create it and write your question into it first."
        )
        exit(0)
    with open(file, "r") as f:
        question = f.read()
    return question


def read_question_from_editor() -> str:
    editor = os.environ.get("EDITOR")
    if not editor:
        editor = "vi"
    with tempfile.NamedTemporaryFile(
        suffix=".txt", delete=False, mode="w+"
    ) as temp_file:
        temp_filename = temp_file.name
        try:
            subprocess.run([editor, temp_filename], check=True)
        except subprocess.CalledProcessError as e:
            print(f"Error launching editor: {e}")
            return ""
        temp_file.close()
        try:
            with open(temp_filename, "r") as f:
                edited_text = f.read()
        except IOError as e:
            print(f"Error reading temp file after close: {e}")
            return ""
        try:
            os.remove(temp_filename)
        except OSError as e:
            print(f"Error deleting temporary file: {e}")
        return edited_text


def choose_question(argument, file, editor) -> str:
    if argument:
        return argument
    if editor:
        return read_question_from_editor()
    if file:
        return read_question_from_file(file)
    print("no question provided, use -q, -f or -e")
    exit(0)


def history_print():
    with open(get_conversation_history_path(), "r") as f:
        print(f.read())


def history_clean():
    os.remove(get_conversation_history_path())


def get_conversation_history() -> list[HistoryElement]:
    path = get_conversation_history_path()
    data = None
    with open(path, "r") as f:
        data = json.load(f)
    return data


def conversation_history_append(question: str, response: str):
    data = get_conversation_history()
    data.append({"question": question, "response": response})
    with open(get_conversation_history_path(), "w") as f:
        json.dump(data, f, indent=4)


def history_append(question: str, response: str):
    with open(get_history_path(), "a") as f:
        f.writelines(
            [
                "\n",
                f"DATE {datetime.now().isoformat()}\n",
                "QUESTION\n",
                f"{question}\n",
                "RESPONSE\n",
                f"{response}\n",
            ]
        )


def configure(api_key: str):
    with open(get_api_key_path(), "+w") as f:
        f.write(api_key)
    print(f"Api key saved to {get_api_key_path()}")


def parse_arguments():
    parser = argparse.ArgumentParser(
        f"""

    {app_name} is a command line interface to interact with your chosen AI assistant.

    Configure ai-cli with your token goten from open_router.
    $ {app_name} --configure $OPEN_ROUTER_API_KEY

    Ask a question to a given model
    $ {app_name} --model "openai" --attitude "you are a monk" --question "what is the meaning of life ?"

    Ask a question written to a file without using the chat history
    $ {app_name} --history-no --file "path_to_a_file_with_a_question_within"

    Ask a question using your favorite editor.
    $ {app_name} --editor

    Print and then delete your chat history
    $ {app_name} --history-print ; {app_name} --history-clean

    Note: currently only a single history is supported.
        """
    )
    parser.add_argument(
        "-m",
        "--model",
        help="AI assistant model: claude, openai, codestral, or the real name gotten from open-router",
        default="claude",
    )
    parser.add_argument(
        "-a",
        "--attitude",
        help="attitude (role) that your AI assistant will endorse",
        default="backend-developer-concise",
    )
    parser.add_argument("-q", "--question", help="question to as to the AI assistant")
    parser.add_argument(
        "-f",
        "--file",
        help="read a question from a file",
        default=f"{get_config_path()}/question.txt",
    )
    parser.add_argument(
        "-e", "--editor", action="store_true", help="open $EDITOR to enter a question"
    )
    parser.add_argument(
        "-hc",
        "--history-clean",
        action="store_true",
        help="clean history of previous message (conversation context) return without asking question to the llm even if question is provided",
    )
    parser.add_argument(
        "-hn",
        "--history-no",
        action="store_true",
        help="ask a question to the llm without the history of the previous messages",
    )
    parser.add_argument(
        "-hp",
        "--history-print",
        action="store_true",
        help="print the conversation history",
    )
    parser.add_argument(
        "-c",
        "--configure",
        help=f"Configure {app_name} by providing your token. example: {app_name} -c $OPEN_ROUTER_API_TOKEN",
    )
    args = parser.parse_args()
    return args


if __name__ == "__main__":
    args = parse_arguments()
    if args.configure:
        configure(args.configure)
        exit(0)
    if args.history_print:
        history_print()
        exit(0)
    if args.history_clean:
        history_clean()
        exit(0)
    question = choose_question(args.question, args.file, args.editor)
    if question == "":
        print("no question provided")
        exit(0)
    model = select_model(args.model)
    attitude = select_attitude(args.attitude)
    print(f"Model: {model}")
    print(f"Question: {question}")
    print("Response:")
    history = None
    if not args.history_no:
        history = get_conversation_history()
    response = ai_cli(question, model, attitude, history)
    console = Console()
    md = Markdown(response)
    console.print(md)
    if not args.history_no:
        conversation_history_append(question, response)
    history_append(question, response)
    print()
